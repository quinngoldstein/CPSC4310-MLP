{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and training-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "0           0  7129300520  20141013T000000  221900.0         3       1.00   \n",
      "1           1  6414100192  20141209T000000  538000.0         3       2.25   \n",
      "2           2  5631500400  20150225T000000  180000.0         2       1.00   \n",
      "3           3  2487200875  20141209T000000  604000.0         4       3.00   \n",
      "4           4  1954400510  20150218T000000  510000.0         3       2.00   \n",
      "\n",
      "   sqft_living  sqft_lot  floors  waterfront  ...  yr_built  yr_renovated  \\\n",
      "0         1180      5650     1.0           0  ...      1955             0   \n",
      "1         2570      7242     2.0           0  ...      1951          1991   \n",
      "2          770     10000     1.0           0  ...      1933             0   \n",
      "3         1960      5000     1.0           0  ...      1965             0   \n",
      "4         1680      8080     1.0           0  ...      1987             0   \n",
      "\n",
      "   zipcode      lat     long  sqft_living15  sqft_lot15   date_time  \\\n",
      "0    98178  47.5112 -122.257           1340        5650  2014-10-13   \n",
      "1    98125  47.7210 -122.319           1690        7639  2014-12-09   \n",
      "2    98028  47.7379 -122.233           2720        8062  2015-02-25   \n",
      "3    98136  47.5208 -122.393           1360        5000  2014-12-09   \n",
      "4    98074  47.6168 -122.045           1800        7503  2015-02-18   \n",
      "\n",
      "   most_recent  price_range  \n",
      "0         1955            0  \n",
      "1         1991            0  \n",
      "2         1933            0  \n",
      "3         1965            1  \n",
      "4         1987            0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "       Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "21608       21608   263000018  20140521T000000  360000.0         3       2.50   \n",
      "21609       21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
      "21610       21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
      "21611       21611   291310100  20150116T000000  400000.0         3       2.50   \n",
      "21612       21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
      "\n",
      "       sqft_living  sqft_lot  floors  waterfront  ...  yr_built  yr_renovated  \\\n",
      "21608         1530      1131     3.0           0  ...      2009             0   \n",
      "21609         2310      5813     2.0           0  ...      2014             0   \n",
      "21610         1020      1350     2.0           0  ...      2009             0   \n",
      "21611         1600      2388     2.0           0  ...      2004             0   \n",
      "21612         1020      1076     2.0           0  ...      2008             0   \n",
      "\n",
      "       zipcode      lat     long  sqft_living15  sqft_lot15   date_time  \\\n",
      "21608    98103  47.6993 -122.346           1530        1509  2014-05-21   \n",
      "21609    98146  47.5107 -122.362           1830        7200  2015-02-23   \n",
      "21610    98144  47.5944 -122.299           1020        2007  2014-06-23   \n",
      "21611    98027  47.5345 -122.069           1410        1287  2015-01-16   \n",
      "21612    98144  47.5941 -122.299           1020        1357  2014-10-15   \n",
      "\n",
      "       most_recent  price_range  \n",
      "21608         2009            0  \n",
      "21609         2014            0  \n",
      "21610         2009            0  \n",
      "21611         2004            0  \n",
      "21612         2008            0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# read data from CSV file to dataframe\n",
    "df = pd.read_csv(r\"./input/kc_sales_cleaned.csv\")\n",
    "\n",
    "# make sure you understand the type of the object\n",
    "print(type(df))\n",
    "\n",
    "# check the top five and the botoom five data tuples\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We use bedrooms, bathrooms, sqft_living, sqft_lot, and 'most_recent' attributes\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Under', 'Over']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.83      0.83      1360\n",
      "        Over       0.71      0.69      0.70       802\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.76      0.76      0.76      2162\n",
      "weighted avg       0.78      0.78      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.84      0.82      1355\n",
      "        Over       0.70      0.66      0.68       807\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.75      0.75      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.83      0.82      1348\n",
      "        Over       0.70      0.66      0.68       814\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.75      0.75      0.75      2162\n",
      "weighted avg       0.76      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.83      0.82      1367\n",
      "        Over       0.69      0.65      0.67       794\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.74      0.74      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1408\n",
      "        Over       0.66      0.66      0.66       753\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.74      0.74      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.84      0.83      1370\n",
      "        Over       0.70      0.67      0.69       791\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.76      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.81      0.82      1394\n",
      "        Over       0.67      0.69      0.68       767\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.83      0.81      1355\n",
      "        Over       0.69      0.66      0.68       806\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.75      0.74      0.75      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.83      0.81      1349\n",
      "        Over       0.69      0.64      0.67       812\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.73      0.74      2161\n",
      "weighted avg       0.75      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1388\n",
      "        Over       0.68      0.68      0.68       773\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "Average precision (ENTROPY): 0.7506297917950204\n",
      "Average recall (ENTROPY): 0.7466249810985011\n",
      "Average accuracy (ENTROPY): 0.768194543674533\n"
     ]
    }
   ],
   "source": [
    "# Entropy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "\n",
    "# decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy', random_state=42)\n",
    "\n",
    "# k-fold cross validation (k=10)\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 2 folds\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "\n",
    "# each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # read to file\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "\n",
    "print(\"Average precision (ENTROPY):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (ENTROPY):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (ENTROPY):\", accuracy_sum/kf.get_n_splits(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.86      0.84      1368\n",
      "        Over       0.74      0.66      0.70       794\n",
      "\n",
      "    accuracy                           0.79      2162\n",
      "   macro avg       0.78      0.76      0.77      2162\n",
      "weighted avg       0.79      0.79      0.79      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.84      0.82      1378\n",
      "        Over       0.70      0.63      0.66       784\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.75      0.74      0.74      2162\n",
      "weighted avg       0.76      0.77      0.76      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.81      0.80      1339\n",
      "        Over       0.68      0.64      0.66       823\n",
      "\n",
      "    accuracy                           0.75      2162\n",
      "   macro avg       0.73      0.73      0.73      2162\n",
      "weighted avg       0.75      0.75      0.75      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.83      0.82      1361\n",
      "        Over       0.70      0.68      0.69       800\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.75      0.76      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1372\n",
      "        Over       0.69      0.68      0.68       789\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.85      0.82      1370\n",
      "        Over       0.70      0.63      0.66       791\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.74      0.74      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.81      0.81      1411\n",
      "        Over       0.64      0.66      0.65       750\n",
      "\n",
      "    accuracy                           0.75      2161\n",
      "   macro avg       0.73      0.73      0.73      2161\n",
      "weighted avg       0.76      0.75      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1348\n",
      "        Over       0.73      0.61      0.67       813\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1381\n",
      "        Over       0.71      0.62      0.66       780\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1366\n",
      "        Over       0.73      0.63      0.67       795\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "Average precision (GINI): 0.7527015625819437\n",
      "Average recall (GINI): 0.7428066335756423\n",
      "Average accuracy (GINI): 0.7688892446665105\n"
     ]
    }
   ],
   "source": [
    "# Gini\n",
    "\n",
    "# Construct a decision tree using gini index\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='gini', random_state=42)\n",
    "\n",
    "# 10-fold CV\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # read to file\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "\n",
    "print(\"Average precision (GINI):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (GINI):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (GINI):\", accuracy_sum/kf.get_n_splits(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance information for gini classifier on max depth =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.84      0.83      2726\n",
      "        Over       0.72      0.70      0.71      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.77      0.77      0.77      4323\n",
      "weighted avg       0.79      0.79      0.79      4323\n",
      "\n",
      "Precision (GINI): 0.7715737156931738\n",
      "Recall (GINI): 0.7694788145968849\n",
      "Accuracy (GINI): 0.7869535045107564\n",
      "\n",
      "Performance information for entropy classifier on max depth =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.84      0.81      0.83      2726\n",
      "        Over       0.69      0.74      0.72      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.78      0.77      4323\n",
      "weighted avg       0.79      0.78      0.79      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7688380414160795\n",
      "Recall (ENTROPY): 0.7757599653789593\n",
      "Accuracy (ENTROPY): 0.7844089752486699\n",
      "\n",
      "Performance information for gini classifier on max depth =  4 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      2726\n",
      "        Over       0.78      0.59      0.67      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (GINI): 0.784653704893585\n",
      "Recall (GINI): 0.7472834014253615\n",
      "Accuracy (GINI): 0.7874161461947722\n",
      "\n",
      "Performance information for entropy classifier on max depth =  4 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      2726\n",
      "        Over       0.78      0.60      0.68      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.79      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7861806827314528\n",
      "Recall (ENTROPY): 0.7494750106927378\n",
      "Accuracy (ENTROPY): 0.7890353920888272\n",
      "\n",
      "Performance information for gini classifier on max depth =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      2726\n",
      "        Over       0.76      0.63      0.69      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (GINI): 0.7792928075916197\n",
      "Recall (GINI): 0.754522649079276\n",
      "Accuracy (GINI): 0.7878787878787878\n",
      "\n",
      "Performance information for entropy classifier on max depth =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.87      0.84      2726\n",
      "        Over       0.74      0.67      0.70      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.77      0.77      4323\n",
      "weighted avg       0.79      0.79      0.79      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7800784522996014\n",
      "Recall (ENTROPY): 0.7664360358357173\n",
      "Accuracy (ENTROPY): 0.7922738838769373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change max depth and observe results \n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)\n",
    "\n",
    "for i in [3,4,5]:\n",
    "    tree_clf_gini = DecisionTreeClassifier(max_depth= i, criterion=\"gini\", random_state=42)\n",
    "    tree_clf_entropy = DecisionTreeClassifier(max_depth = i, criterion=\"entropy\", random_state=42)\n",
    "    \n",
    "    tree_clf_gini.fit(X_train, y_train)\n",
    "    tree_clf_entropy.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_gini = tree_clf_gini.predict(X_test)\n",
    "    \n",
    "    y_pred_entropy = tree_clf_entropy.predict(X_test)    \n",
    "    \n",
    "    print(\"Performance information for gini classifier on max depth = \", i, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_gini, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict_gini = classification_report(y_test, y_pred_gini, target_names=target_names, output_dict=True)\n",
    "    print(\"Precision (GINI):\", result_metrics_dict_gini[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall (GINI):\", result_metrics_dict_gini[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy (GINI):\", result_metrics_dict_gini[\"accuracy\"])\n",
    "    print()\n",
    "    \n",
    "    print(\"Performance information for entropy classifier on max depth = \", i, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_entropy, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict_entropy = classification_report(y_test, y_pred_entropy, target_names=target_names, output_dict=True)\n",
    "    print(\"Precision (ENTROPY):\", result_metrics_dict_entropy[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall (ENTROPY):\", result_metrics_dict_entropy[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy (ENTROPY):\", result_metrics_dict_entropy[\"accuracy\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for K =  3 \n",
      "\n",
      "Average precision (GINI): 0.7842357101370668\n",
      "Average recall (GINI): 0.7543143091297481\n",
      "Average accuracy (GINI): 0.7910518779052375 \n",
      "\n",
      "Average precision (ENTROPY): 0.7784626818032855\n",
      "Average recall (ENTROPY): 0.7620881182030738\n",
      "Average accuracy (ENTROPY): 0.7905891077809986 \n",
      "\n",
      "Results for K =  5 \n",
      "\n",
      "Average precision (GINI): 0.7813917596216193\n",
      "Average recall (GINI): 0.7533246051759319\n",
      "Average accuracy (GINI): 0.7894326195356607 \n",
      "\n",
      "Average precision (ENTROPY): 0.7783766143806481\n",
      "Average recall (ENTROPY): 0.7572549110496708\n",
      "Average accuracy (ENTROPY): 0.7886458717686132 \n",
      "\n",
      "Results for K =  7 \n",
      "\n",
      "Average precision (GINI): 0.7791384270824023\n",
      "Average recall (GINI): 0.7519252266027997\n",
      "Average accuracy (GINI): 0.7874887874151459 \n",
      "\n",
      "Average precision (ENTROPY): 0.7776391119890638\n",
      "Average recall (ENTROPY): 0.7586305424816775\n",
      "Average accuracy (ENTROPY): 0.7888305360916352 \n",
      "\n",
      "Results for K =  10 \n",
      "\n",
      "Average precision (GINI): 0.7787629914134627\n",
      "Average recall (GINI): 0.7540267001654275\n",
      "Average accuracy (GINI): 0.7880897424317468 \n",
      "\n",
      "Average precision (ENTROPY): 0.777115373088708\n",
      "Average recall (ENTROPY): 0.7562798880279982\n",
      "Average accuracy (ENTROPY): 0.7874883403159447 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change k value and observe results\n",
    "\n",
    "for k in [3,5,7,10]:\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    precision_sum_gini = recall_sum_gini = accuracy_sum_gini = 0\n",
    "    precision_sum_entropy = recall_sum_entropy = accuracy_sum_entropy = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        tree_clf_gini = DecisionTreeClassifier(max_depth= i, criterion=\"gini\", random_state=42)\n",
    "        tree_clf_entropy = DecisionTreeClassifier(max_depth = i, criterion=\"entropy\", random_state=42)\n",
    "    \n",
    "        tree_clf_gini.fit(X_train, y_train)\n",
    "        tree_clf_entropy.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred_gini = tree_clf_gini.predict(X_test)\n",
    "        y_pred_entropy = tree_clf_entropy.predict(X_test)  \n",
    "        \n",
    "    \n",
    "        result_metrics_dict_gini = classification_report(y_test, y_pred_gini, target_names=target_names, output_dict=True)\n",
    "        result_metrics_dict_entropy = classification_report(y_test, y_pred_entropy, target_names=target_names, output_dict=True)\n",
    "    \n",
    "        precision_sum_gini += result_metrics_dict_gini[\"macro avg\"][\"precision\"]\n",
    "        recall_sum_gini += result_metrics_dict_gini[\"macro avg\"][\"recall\"]\n",
    "        accuracy_sum_gini += result_metrics_dict_gini[\"accuracy\"]\n",
    "        \n",
    "        precision_sum_entropy += result_metrics_dict_entropy[\"macro avg\"][\"precision\"]\n",
    "        recall_sum_entropy += result_metrics_dict_entropy[\"macro avg\"][\"recall\"]\n",
    "        accuracy_sum_entropy += result_metrics_dict_entropy[\"accuracy\"]\n",
    "        \n",
    "    print(\"Results for K = \", k, \"\\n\")\n",
    "    \n",
    "    print(\"Average precision (GINI):\", precision_sum_gini/kf.get_n_splits(X))\n",
    "    print(\"Average recall (GINI):\", recall_sum_gini/kf.get_n_splits(X))\n",
    "    print(\"Average accuracy (GINI):\", accuracy_sum_gini/kf.get_n_splits(X), \"\\n\")\n",
    "    \n",
    "    print(\"Average precision (ENTROPY):\", precision_sum_entropy/kf.get_n_splits(X))\n",
    "    print(\"Average recall (ENTROPY):\", recall_sum_entropy/kf.get_n_splits(X))\n",
    "    print(\"Average accuracy (ENTROPY):\", accuracy_sum_entropy/kf.get_n_splits(X), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The performance of the gini and entropy classifiers was actually fairly similar on the same testing set, and the best performing depth appeared to be max depth = 4. Changing the value of K didn't change the results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot important features\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT classifier on training set: 1.00\n",
      "Accuracy of DT classifier on test set: 0.74\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Find the important features of the entropy DT model\n",
    "clf1 = DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(clf1.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(clf1.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=60)\n",
    "\n",
    "# call the function above\n",
    "plot_feature_importances(clf1, X.columns.values.tolist())\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(tree_clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: kNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use bedrooms, bathrooms, sqft_living, sqft_lot, and 'most_recent' attributes\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1\n",
      "[[2179  547]\n",
      " [ 634  963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.77      0.80      0.79      2726\n",
      "        Over       0.64      0.60      0.62      1597\n",
      "\n",
      "    accuracy                           0.73      4323\n",
      "   macro avg       0.71      0.70      0.70      4323\n",
      "weighted avg       0.72      0.73      0.73      4323\n",
      "\n",
      "k=5\n",
      "[[2355  371]\n",
      " [ 617  980]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.83      2726\n",
      "        Over       0.73      0.61      0.66      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=15\n",
      "[[2364  362]\n",
      " [ 615  982]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      2726\n",
      "        Over       0.73      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=20\n",
      "[[2422  304]\n",
      " [ 675  922]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.89      0.83      2726\n",
      "        Over       0.75      0.58      0.65      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.77      0.73      0.74      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=25\n",
      "[[2389  337]\n",
      " [ 619  978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      2726\n",
      "        Over       0.74      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.78      0.78      0.77      4323\n",
      "\n",
      "k=30\n",
      "[[2417  309]\n",
      " [ 658  939]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.89      0.83      2726\n",
      "        Over       0.75      0.59      0.66      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.77      0.78      0.77      4323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the best k value\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_value = [1, 5, 15, 20, 25, 30]\n",
    "\n",
    "for k in k_value:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"k={k}\")\n",
    "    \n",
    "    # plot a confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    # Print classification report\n",
    "    target_names = ['Under', 'Over']\n",
    "   \n",
    "    #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    result_metrics = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(result_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "\n",
    "# create k-fold validation\n",
    "kf = KFold(n_splits=3, random_state=None, shuffle=True) # Define the split - into 3 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg precision (weighted): 0.7665288961960375\n",
      "Avg recall (weighted): 0.7704620881066537\n",
      "Accuracy: 0.7704620881066537\n"
     ]
    }
   ],
   "source": [
    "# change DataFrame to numpy array\n",
    "XX = X.to_numpy()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 30)\n",
    "\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "for train_index, test_index in kf.split(XX):\n",
    "    X_train, X_test = XX[train_index], XX[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # read to file\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"weighted avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"weighted avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "\n",
    "print(\"Avg precision (weighted):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Avg recall (weighted):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Accuracy:\", accuracy_sum/kf.get_n_splits(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data into training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "\n",
    "# We start with k=3 and will increase it to 10.\n",
    "kf = KFold(n_splits=3, random_state=None, shuffle=True) # Define the split - into 10 folds \n",
    "\n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "print (kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GaussianNB classifier on training set: 0.75\n",
      "Accuracy of GaussianNB classifier on test set: 0.77\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Apply k-cross\n",
    "nbclf = GaussianNB()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # for each iteration, get training data and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # train the model using training data\n",
    "    nbclf.fit(X_train, y_train)\n",
    "    \n",
    "    # show how model performs with training data and test data\n",
    "    print('Accuracy of GaussianNB classifier on training set: {:.2f}'\n",
    "         .format(nbclf.score(X_train, y_train)))\n",
    "\n",
    "    print('Accuracy of GaussianNB classifier on test set: {:.2f}'\n",
    "         .format(nbclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4223  316]\n",
      " [1419 1247]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.75      0.93      0.83      4539\n",
      "        Over       0.80      0.47      0.59      2666\n",
      "\n",
      "    accuracy                           0.76      7205\n",
      "   macro avg       0.77      0.70      0.71      7205\n",
      "weighted avg       0.77      0.76      0.74      7205\n",
      "\n",
      "{'Under': {'precision': 0.7484934420418291, 'recall': 0.9303811412205332, 'f1-score': 0.8295845201846577, 'support': 4539}, 'Over': {'precision': 0.7978246960972489, 'recall': 0.46774193548387094, 'f1-score': 0.5897375266020336, 'support': 2666}, 'accuracy': 0.7591950034698126, 'macro avg': {'precision': 0.773159069069539, 'recall': 0.6990615383522021, 'f1-score': 0.7096610233933456, 'support': 7205}, 'weighted avg': {'precision': 0.7667470330635846, 'recall': 0.7591950034698126, 'f1-score': 0.740836139214321, 'support': 7205}}\n",
      "[[4239  344]\n",
      " [1448 1173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.75      0.92      0.83      4583\n",
      "        Over       0.77      0.45      0.57      2621\n",
      "\n",
      "    accuracy                           0.75      7204\n",
      "   macro avg       0.76      0.69      0.70      7204\n",
      "weighted avg       0.76      0.75      0.73      7204\n",
      "\n",
      "{'Under': {'precision': 0.7453842096008441, 'recall': 0.9249399956360462, 'f1-score': 0.8255111976630963, 'support': 4583}, 'Over': {'precision': 0.7732366512854317, 'recall': 0.44753910721098816, 'f1-score': 0.5669405509908169, 'support': 2621}, 'accuracy': 0.7512493059411438, 'macro avg': {'precision': 0.759310430443138, 'recall': 0.6862395514235172, 'f1-score': 0.6962258743269566, 'support': 7204}, 'weighted avg': {'precision': 0.755517642368099, 'recall': 0.7512493059411438, 'f1-score': 0.7314365634421018, 'support': 7204}}\n",
      "[[4207  365]\n",
      " [1380 1252]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.75      0.92      0.83      4572\n",
      "        Over       0.77      0.48      0.59      2632\n",
      "\n",
      "    accuracy                           0.76      7204\n",
      "   macro avg       0.76      0.70      0.71      7204\n",
      "weighted avg       0.76      0.76      0.74      7204\n",
      "\n",
      "{'Under': {'precision': 0.7529980311437265, 'recall': 0.9201662292213474, 'f1-score': 0.8282311251107393, 'support': 4572}, 'Over': {'precision': 0.7742733457019171, 'recall': 0.4756838905775076, 'f1-score': 0.5893151329724641, 'support': 2632}, 'accuracy': 0.7577734591893392, 'macro avg': {'precision': 0.7636356884228218, 'recall': 0.6979250598994275, 'f1-score': 0.7087731290416017, 'support': 7204}, 'weighted avg': {'precision': 0.7607710222482736, 'recall': 0.7577734591893392, 'f1-score': 0.7409425505260724, 'support': 7204}}\n"
     ]
    }
   ],
   "source": [
    "# Model performance using k-cross\n",
    "nbclf2 = GaussianNB()\n",
    "\n",
    "# !!!!! Please make a summary of the model performance (averaging k folds' results) using result_metrics_dict \n",
    "for train_index, test_index in kf.split(X):\n",
    "    # for each iteration, get training data and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # train the model using training data\n",
    "    nbclf2.fit(X_train, y_train)\n",
    "    \n",
    "    # predict y values using test data\n",
    "    y_pred = nbclf2.predict(X_test)\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    # read to file\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Since we can retrieve a dictionary of metrics and access the values using dictionary,\n",
    "    # now we can sum of the results of each iteration and get the average\n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    print(result_metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8214880376880951\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "y_score = nbclf2.predict_proba(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_score[:,1])\n",
    "\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('Accuracy = ', roc_auc)\n",
    "\n",
    "# Plotting\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=('Accuracy = %0.2f'%roc_auc))\n",
    "plt.legend(loc='lower right', prop={'size':8})\n",
    "plt.plot([0,1],[0,1], color='lightgrey', linestyle='--')\n",
    "plt.xlim([-0.05,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
