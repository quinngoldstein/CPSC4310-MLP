{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and training-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "0           0  7129300520  20141013T000000  221900.0         3       1.00   \n",
      "1           1  6414100192  20141209T000000  538000.0         3       2.25   \n",
      "2           2  5631500400  20150225T000000  180000.0         2       1.00   \n",
      "3           3  2487200875  20141209T000000  604000.0         4       3.00   \n",
      "4           4  1954400510  20150218T000000  510000.0         3       2.00   \n",
      "\n",
      "   sqft_living  sqft_lot  floors  waterfront  ...  yr_built  yr_renovated  \\\n",
      "0         1180      5650     1.0           0  ...      1955             0   \n",
      "1         2570      7242     2.0           0  ...      1951          1991   \n",
      "2          770     10000     1.0           0  ...      1933             0   \n",
      "3         1960      5000     1.0           0  ...      1965             0   \n",
      "4         1680      8080     1.0           0  ...      1987             0   \n",
      "\n",
      "   zipcode      lat     long  sqft_living15  sqft_lot15   date_time  \\\n",
      "0    98178  47.5112 -122.257           1340        5650  2014-10-13   \n",
      "1    98125  47.7210 -122.319           1690        7639  2014-12-09   \n",
      "2    98028  47.7379 -122.233           2720        8062  2015-02-25   \n",
      "3    98136  47.5208 -122.393           1360        5000  2014-12-09   \n",
      "4    98074  47.6168 -122.045           1800        7503  2015-02-18   \n",
      "\n",
      "   most_recent  price_range  \n",
      "0         1955            0  \n",
      "1         1991            0  \n",
      "2         1933            0  \n",
      "3         1965            1  \n",
      "4         1987            0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "       Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "21608       21608   263000018  20140521T000000  360000.0         3       2.50   \n",
      "21609       21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
      "21610       21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
      "21611       21611   291310100  20150116T000000  400000.0         3       2.50   \n",
      "21612       21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
      "\n",
      "       sqft_living  sqft_lot  floors  waterfront  ...  yr_built  yr_renovated  \\\n",
      "21608         1530      1131     3.0           0  ...      2009             0   \n",
      "21609         2310      5813     2.0           0  ...      2014             0   \n",
      "21610         1020      1350     2.0           0  ...      2009             0   \n",
      "21611         1600      2388     2.0           0  ...      2004             0   \n",
      "21612         1020      1076     2.0           0  ...      2008             0   \n",
      "\n",
      "       zipcode      lat     long  sqft_living15  sqft_lot15   date_time  \\\n",
      "21608    98103  47.6993 -122.346           1530        1509  2014-05-21   \n",
      "21609    98146  47.5107 -122.362           1830        7200  2015-02-23   \n",
      "21610    98144  47.5944 -122.299           1020        2007  2014-06-23   \n",
      "21611    98027  47.5345 -122.069           1410        1287  2015-01-16   \n",
      "21612    98144  47.5941 -122.299           1020        1357  2014-10-15   \n",
      "\n",
      "       most_recent  price_range  \n",
      "21608         2009            0  \n",
      "21609         2014            0  \n",
      "21610         2009            0  \n",
      "21611         2004            0  \n",
      "21612         2008            0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# read data from CSV file to dataframe\n",
    "df = pd.read_csv(r\"./input/kc_sales_cleaned.csv\")\n",
    "\n",
    "# make sure you understand the type of the object\n",
    "print(type(df))\n",
    "\n",
    "# check the top five and the botoom five data tuples\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We use bedrooms, bathrooms, sqft_living, sqft_lot, and 'most_recent' attributes\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Under', 'Over']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.82      0.81      1337\n",
      "        Over       0.70      0.69      0.69       825\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.75      0.75      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.84      0.83      1368\n",
      "        Over       0.71      0.69      0.70       794\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.76      0.77      2162\n",
      "weighted avg       0.78      0.78      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.83      0.81      1370\n",
      "        Over       0.68      0.62      0.65       792\n",
      "\n",
      "    accuracy                           0.75      2162\n",
      "   macro avg       0.73      0.73      0.73      2162\n",
      "weighted avg       0.75      0.75      0.75      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.84      0.83      1382\n",
      "        Over       0.70      0.67      0.69       779\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.83      0.82      1376\n",
      "        Over       0.68      0.64      0.66       785\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.74      0.74      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.82      0.82      1357\n",
      "        Over       0.69      0.67      0.68       804\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.84      0.82      1376\n",
      "        Over       0.70      0.66      0.68       785\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.83      0.82      1379\n",
      "        Over       0.69      0.66      0.68       782\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.82      0.82      1385\n",
      "        Over       0.67      0.66      0.66       776\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.74      0.74      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.82      0.82      1364\n",
      "        Over       0.69      0.66      0.68       797\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.75      0.74      0.75      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "Average precision (ENTROPY): 0.7502878529820852\n",
      "Average recall (ENTROPY): 0.74556591208894\n",
      "Average accuracy (ENTROPY): 0.7679173867239488\n"
     ]
    }
   ],
   "source": [
    "# Entropy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "\n",
    "# decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy', random_state=42)\n",
    "\n",
    "# k-fold cross validation (k=10)\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 2 folds\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "# each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    # Store metrics data for later analysis\n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision (ENTROPY):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (ENTROPY):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (ENTROPY):\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "metrics_df.to_csv('./MLP3_data/entropy_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1364\n",
      "        Over       0.70      0.69      0.69       798\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.75      0.76      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.85      0.82      1371\n",
      "        Over       0.70      0.61      0.65       791\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.75      0.73      0.74      2162\n",
      "weighted avg       0.76      0.76      0.76      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.85      0.82      1354\n",
      "        Over       0.71      0.62      0.66       808\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.75      0.73      0.74      2162\n",
      "weighted avg       0.76      0.76      0.76      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1372\n",
      "        Over       0.69      0.68      0.68       789\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.85      0.82      1392\n",
      "        Over       0.69      0.60      0.64       769\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.72      0.73      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.84      0.82      1366\n",
      "        Over       0.70      0.65      0.68       795\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.85      0.82      1381\n",
      "        Over       0.70      0.61      0.65       780\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.75      0.73      0.74      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.84      0.82      1363\n",
      "        Over       0.70      0.65      0.68       798\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1360\n",
      "        Over       0.72      0.65      0.68       801\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.75      0.76      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.85      0.83      1371\n",
      "        Over       0.72      0.66      0.68       790\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "Average precision (GINI): 0.752781393962894\n",
      "Average recall (GINI): 0.7420083202554701\n",
      "Average accuracy (GINI): 0.7691208330675704\n"
     ]
    }
   ],
   "source": [
    "# Gini\n",
    "\n",
    "# Construct a decision tree using gini index\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='gini', random_state=42)\n",
    "\n",
    "# 10-fold CV\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # read to file\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision (GINI):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (GINI):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (GINI):\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "metrics_df.to_csv('./MLP3_data/gini_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance information for gini classifier on max depth =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.84      0.83      2726\n",
      "        Over       0.72      0.70      0.71      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.77      0.77      0.77      4323\n",
      "weighted avg       0.79      0.79      0.79      4323\n",
      "\n",
      "Precision (GINI): 0.7715737156931738\n",
      "Recall (GINI): 0.7694788145968849\n",
      "Accuracy (GINI): 0.7869535045107564\n",
      "\n",
      "Performance information for entropy classifier on max depth =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.84      0.81      0.83      2726\n",
      "        Over       0.69      0.74      0.72      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.78      0.77      4323\n",
      "weighted avg       0.79      0.78      0.79      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7688380414160795\n",
      "Recall (ENTROPY): 0.7757599653789593\n",
      "Accuracy (ENTROPY): 0.7844089752486699\n",
      "\n",
      "Performance information for gini classifier on max depth =  4 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      2726\n",
      "        Over       0.78      0.59      0.67      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (GINI): 0.784653704893585\n",
      "Recall (GINI): 0.7472834014253615\n",
      "Accuracy (GINI): 0.7874161461947722\n",
      "\n",
      "Performance information for entropy classifier on max depth =  4 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      2726\n",
      "        Over       0.78      0.60      0.68      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.79      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7861806827314528\n",
      "Recall (ENTROPY): 0.7494750106927378\n",
      "Accuracy (ENTROPY): 0.7890353920888272\n",
      "\n",
      "Performance information for gini classifier on max depth =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      2726\n",
      "        Over       0.76      0.63      0.69      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (GINI): 0.7792928075916197\n",
      "Recall (GINI): 0.754522649079276\n",
      "Accuracy (GINI): 0.7878787878787878\n",
      "\n",
      "Performance information for entropy classifier on max depth =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.87      0.84      2726\n",
      "        Over       0.74      0.67      0.70      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.77      0.77      4323\n",
      "weighted avg       0.79      0.79      0.79      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7800784522996014\n",
      "Recall (ENTROPY): 0.7664360358357173\n",
      "Accuracy (ENTROPY): 0.7922738838769373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change max depth and observe results \n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)\n",
    "\n",
    "gini_metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "entropy_metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for i in [3,4,5]:\n",
    "    tree_clf_gini = DecisionTreeClassifier(max_depth= i, criterion=\"gini\", random_state=42)\n",
    "    tree_clf_entropy = DecisionTreeClassifier(max_depth = i, criterion=\"entropy\", random_state=42)\n",
    "    \n",
    "    tree_clf_gini.fit(X_train, y_train)\n",
    "    tree_clf_entropy.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_gini = tree_clf_gini.predict(X_test)\n",
    "    \n",
    "    y_pred_entropy = tree_clf_entropy.predict(X_test)    \n",
    "    \n",
    "    print(\"Performance information for gini classifier on max depth = \", i, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_gini, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict_gini = classification_report(y_test, y_pred_gini, target_names=target_names, output_dict=True)\n",
    "    print(\"Precision (GINI):\", result_metrics_dict_gini[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall (GINI):\", result_metrics_dict_gini[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy (GINI):\", result_metrics_dict_gini[\"accuracy\"])\n",
    "    print()\n",
    "    \n",
    "    print(\"Performance information for entropy classifier on max depth = \", i, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_entropy, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict_entropy = classification_report(y_test, y_pred_entropy, target_names=target_names, output_dict=True)\n",
    "    print(\"Precision (ENTROPY):\", result_metrics_dict_entropy[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall (ENTROPY):\", result_metrics_dict_entropy[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy (ENTROPY):\", result_metrics_dict_entropy[\"accuracy\"])\n",
    "    print()\n",
    "    \n",
    "    gini_metrics_list = []\n",
    "    gini_metrics_list.append(result_metrics_dict_gini[\"macro avg\"][\"precision\"])\n",
    "    gini_metrics_list.append(result_metrics_dict_gini[\"macro avg\"][\"recall\"])\n",
    "    gini_metrics_list.append(result_metrics_dict_gini[\"accuracy\"])\n",
    "    \n",
    "    gini_metrics_series = pd.Series(gini_metrics_list, index=gini_metrics_df.columns)\n",
    "    gini_metrics_series.name = i\n",
    "    gini_metrics_df = gini_metrics_df.append(gini_metrics_series)\n",
    "    \n",
    "    entropy_metrics_list = []\n",
    "    entropy_metrics_list.append(result_metrics_dict_entropy[\"macro avg\"][\"precision\"])\n",
    "    entropy_metrics_list.append(result_metrics_dict_entropy[\"macro avg\"][\"recall\"])\n",
    "    entropy_metrics_list.append(result_metrics_dict_entropy[\"accuracy\"])\n",
    "    \n",
    "    entropy_metrics_series = pd.Series(entropy_metrics_list, index=entropy_metrics_df.columns)\n",
    "    entropy_metrics_series.name = i\n",
    "    entropy_metrics_df = entropy_metrics_df.append(entropy_metrics_series)\n",
    "      \n",
    "gini_metrics_df.to_csv('./MLP3_data/gini_depth_metrics.csv')\n",
    "entropy_metrics_df.to_csv('./MLP3_data/entropy_depth_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for K =  3 \n",
      "\n",
      "Average precision (GINI): 0.7778643726545081\n",
      "Average recall (GINI): 0.7591647168172875\n",
      "Average accuracy (GINI): 0.789108243383434 \n",
      "\n",
      "Average precision (ENTROPY): 0.7797965676533513\n",
      "Average recall (ENTROPY): 0.7549891414953288\n",
      "Average accuracy (ENTROPY): 0.7884143836095889 \n",
      "\n",
      "Results for K =  5 \n",
      "\n",
      "Average precision (GINI): 0.7784578512569724\n",
      "Average recall (GINI): 0.7617115677026007\n",
      "Average accuracy (GINI): 0.7905887848676563 \n",
      "\n",
      "Average precision (ENTROPY): 0.7775178803230853\n",
      "Average recall (ENTROPY): 0.7554042584266829\n",
      "Average accuracy (ENTROPY): 0.7878586851235222 \n",
      "\n",
      "Results for K =  7 \n",
      "\n",
      "Average precision (GINI): 0.7782504800583844\n",
      "Average recall (GINI): 0.7556461808671922\n",
      "Average accuracy (GINI): 0.7885078857950427 \n",
      "\n",
      "Average precision (ENTROPY): 0.7779967938596112\n",
      "Average recall (ENTROPY): 0.7590108125531908\n",
      "Average accuracy (ENTROPY): 0.7888783117138153 \n",
      "\n",
      "Results for K =  10 \n",
      "\n",
      "Average precision (GINI): 0.7769933116733166\n",
      "Average recall (GINI): 0.7542627026146336\n",
      "Average accuracy (GINI): 0.787443392474704 \n",
      "\n",
      "Average precision (ENTROPY): 0.7775677582018463\n",
      "Average recall (ENTROPY): 0.7590088561095266\n",
      "Average accuracy (ENTROPY): 0.7888314888308895 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change k value and observe results\n",
    "\n",
    "for k in [3,5,7,10]:\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    precision_sum_gini = recall_sum_gini = accuracy_sum_gini = 0\n",
    "    precision_sum_entropy = recall_sum_entropy = accuracy_sum_entropy = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        tree_clf_gini = DecisionTreeClassifier(max_depth= i, criterion=\"gini\", random_state=42)\n",
    "        tree_clf_entropy = DecisionTreeClassifier(max_depth = i, criterion=\"entropy\", random_state=42)\n",
    "    \n",
    "        tree_clf_gini.fit(X_train, y_train)\n",
    "        tree_clf_entropy.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred_gini = tree_clf_gini.predict(X_test)\n",
    "        y_pred_entropy = tree_clf_entropy.predict(X_test)  \n",
    "        \n",
    "    \n",
    "        result_metrics_dict_gini = classification_report(y_test, y_pred_gini, target_names=target_names, output_dict=True)\n",
    "        result_metrics_dict_entropy = classification_report(y_test, y_pred_entropy, target_names=target_names, output_dict=True)\n",
    "    \n",
    "        precision_sum_gini += result_metrics_dict_gini[\"macro avg\"][\"precision\"]\n",
    "        recall_sum_gini += result_metrics_dict_gini[\"macro avg\"][\"recall\"]\n",
    "        accuracy_sum_gini += result_metrics_dict_gini[\"accuracy\"]\n",
    "        \n",
    "        precision_sum_entropy += result_metrics_dict_entropy[\"macro avg\"][\"precision\"]\n",
    "        recall_sum_entropy += result_metrics_dict_entropy[\"macro avg\"][\"recall\"]\n",
    "        accuracy_sum_entropy += result_metrics_dict_entropy[\"accuracy\"]\n",
    "        \n",
    "    print(\"Results for K = \", k, \"\\n\")\n",
    "    \n",
    "    print(\"Average precision (GINI):\", precision_sum_gini/kf.get_n_splits(X))\n",
    "    print(\"Average recall (GINI):\", recall_sum_gini/kf.get_n_splits(X))\n",
    "    print(\"Average accuracy (GINI):\", accuracy_sum_gini/kf.get_n_splits(X), \"\\n\")\n",
    "    \n",
    "    print(\"Average precision (ENTROPY):\", precision_sum_entropy/kf.get_n_splits(X))\n",
    "    print(\"Average recall (ENTROPY):\", recall_sum_entropy/kf.get_n_splits(X))\n",
    "    print(\"Average accuracy (ENTROPY):\", accuracy_sum_entropy/kf.get_n_splits(X), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the gini and entropy classifiers was actually fairly similar on the same testing set, and the best performing depth appeared to be max depth = 5. Changing the value of K didn't change the results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot important features\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT classifier on training set: 1.00\n",
      "Accuracy of DT classifier on test set: 0.73\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Find the important features of the entropy DT model\n",
    "clf1 = DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(clf1.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(clf1.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=60)\n",
    "\n",
    "# call the function above\n",
    "plot_feature_importances(clf1, X.columns.values.tolist())\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(tree_clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider entropy Decision Tree with max depth = 5 and K = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree with max depth = 5 and K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.84      0.88      0.86      1368\n",
      "        Over       0.78      0.70      0.74       794\n",
      "\n",
      "    accuracy                           0.82      2162\n",
      "   macro avg       0.81      0.79      0.80      2162\n",
      "weighted avg       0.81      0.82      0.81      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      1378\n",
      "        Over       0.76      0.58      0.65       784\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.74      0.75      2162\n",
      "weighted avg       0.78      0.78      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.84      0.82      1339\n",
      "        Over       0.72      0.65      0.68       823\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.75      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1361\n",
      "        Over       0.73      0.65      0.68       800\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.85      0.84      1372\n",
      "        Over       0.73      0.69      0.71       789\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.77      0.77      2161\n",
      "weighted avg       0.79      0.79      0.79      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.86      0.84      1370\n",
      "        Over       0.73      0.66      0.70       791\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.77      0.76      0.77      2161\n",
      "weighted avg       0.79      0.79      0.79      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.83      0.83      1411\n",
      "        Over       0.68      0.66      0.67       750\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.87      0.84      1348\n",
      "        Over       0.76      0.66      0.71       813\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.77      0.77      2161\n",
      "weighted avg       0.79      0.79      0.79      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      1381\n",
      "        Over       0.75      0.62      0.68       780\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.75      0.76      2161\n",
      "weighted avg       0.78      0.79      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      1366\n",
      "        Over       0.77      0.58      0.66       795\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.78      0.74      0.75      2161\n",
      "weighted avg       0.78      0.78      0.77      2161\n",
      "\n",
      "Average precision (ENTROPY): 0.7745390262680163\n",
      "Average recall (ENTROPY): 0.756553412400758\n",
      "Average accuracy (ENTROPY): 0.7863319821869564\n"
     ]
    }
   ],
   "source": [
    "# Entropy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "\n",
    "# decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=42)\n",
    "\n",
    "# k-fold cross validation (k=10)\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 2 folds\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "# each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    # Store metrics data for later analysis\n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision (ENTROPY):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (ENTROPY):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (ENTROPY):\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "#os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "metrics_df.to_csv('./MLP3_data/best_tree_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: kNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use bedrooms, bathrooms, sqft_living, sqft_lot, and 'most_recent' attributes\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1\n",
      "[[2179  547]\n",
      " [ 634  963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.77      0.80      0.79      2726\n",
      "        Over       0.64      0.60      0.62      1597\n",
      "\n",
      "    accuracy                           0.73      4323\n",
      "   macro avg       0.71      0.70      0.70      4323\n",
      "weighted avg       0.72      0.73      0.73      4323\n",
      "\n",
      "k=5\n",
      "[[2355  371]\n",
      " [ 617  980]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.83      2726\n",
      "        Over       0.73      0.61      0.66      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=15\n",
      "[[2364  362]\n",
      " [ 615  982]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      2726\n",
      "        Over       0.73      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=20\n",
      "[[2422  304]\n",
      " [ 675  922]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.89      0.83      2726\n",
      "        Over       0.75      0.58      0.65      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.77      0.73      0.74      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=25\n",
      "[[2389  337]\n",
      " [ 619  978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      2726\n",
      "        Over       0.74      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.78      0.78      0.77      4323\n",
      "\n",
      "k=30\n",
      "[[2417  309]\n",
      " [ 658  939]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.89      0.83      2726\n",
      "        Over       0.75      0.59      0.66      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.77      0.78      0.77      4323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DONT USE THIS ONE\n",
    "# find the best k value \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_value = [1, 5, 15, 20, 25, 30]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for k in k_value:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"k={k}\")\n",
    "    \n",
    "    # plot a confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    # Print classification report\n",
    "    target_names = ['Under', 'Over']\n",
    "   \n",
    "    #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    result_metrics = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(result_metrics)\n",
    "    \n",
    "    #metrics_list = []\n",
    "    #metrics_list.append(result_metrics[\"macro avg\"][\"precision\"])\n",
    "    #metrics_list.append(result_metrics[\"macro avg\"][\"recall\"])\n",
    "    #metrics_list.append(result_metrics[\"accuracy\"])\n",
    "    \n",
    "    #metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    #metrics_series.name = k\n",
    "    #metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "#metrics_df.to_csv('./MLP3_data/knn_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance information for KNN classifier on K =  1 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.77      0.80      0.79      2726\n",
      "        Over       0.64      0.60      0.62      1597\n",
      "\n",
      "    accuracy                           0.73      4323\n",
      "   macro avg       0.71      0.70      0.70      4323\n",
      "weighted avg       0.72      0.73      0.72      4323\n",
      "\n",
      "Precision: 0.7059254282407899\n",
      "Recall: 0.7008595766732469\n",
      "Accuracy: 0.7265787647467037\n",
      "\n",
      "Performance information for KNN classifier on K =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.83      0.81      2726\n",
      "        Over       0.68      0.61      0.65      1597\n",
      "\n",
      "    accuracy                           0.75      4323\n",
      "   macro avg       0.74      0.72      0.73      4323\n",
      "weighted avg       0.75      0.75      0.75      4323\n",
      "\n",
      "Precision: 0.7355285119229038\n",
      "Recall: 0.723660605381238\n",
      "Accuracy: 0.7527180198935924\n",
      "\n",
      "Performance information for KNN classifier on K =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.83      2726\n",
      "        Over       0.73      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "Precision: 0.7591957244833127\n",
      "Recall: 0.7389602937643077\n",
      "Accuracy: 0.7716863289382373\n",
      "\n",
      "Performance information for KNN classifier on K =  10 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.89      0.83      2726\n",
      "        Over       0.75      0.57      0.65      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.73      0.74      4323\n",
      "weighted avg       0.77      0.77      0.76      4323\n",
      "\n",
      "Precision: 0.7647698156784744\n",
      "Recall: 0.7292036012130227\n",
      "Accuracy: 0.7709923664122137\n",
      "\n",
      "Performance information for KNN classifier on K =  15 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      2726\n",
      "        Over       0.73      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "Precision: 0.7621048230470435\n",
      "Recall: 0.7410538192713686\n",
      "Accuracy: 0.773999537358316\n",
      "\n",
      "Performance information for KNN classifier on K =  20 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.89      0.83      2726\n",
      "        Over       0.75      0.58      0.65      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.77      0.73      0.74      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "Precision: 0.7670431470543773\n",
      "Recall: 0.732906894851912\n",
      "Accuracy: 0.7735368956743003\n",
      "\n",
      "Performance information for KNN classifier on K =  25 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      2726\n",
      "        Over       0.74      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.78      0.78      0.77      4323\n",
      "\n",
      "Precision: 0.7689708306366799\n",
      "Recall: 0.7443869443394185\n",
      "Accuracy: 0.7788572750404812\n",
      "\n",
      "Performance information for KNN classifier on K =  30 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.89      0.83      2726\n",
      "        Over       0.75      0.59      0.66      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.77      0.78      0.77      4323\n",
      "\n",
      "Precision: 0.7692100531582239\n",
      "Recall: 0.7373122798570871\n",
      "Accuracy: 0.7763127457783946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change K value and observe results \n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "k_values = [1, 3, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "        \n",
    "    #train_score.append(knn.score(X_train, y_train))\n",
    "    #test_score.append(knn.score(X_test, y_test)) \n",
    "    \n",
    "    print(\"Performance information for KNN classifier on K = \", k, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # plot a confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    #print(confusion_mat)\n",
    "    \n",
    "    #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    results = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    #print(results)\n",
    "    \n",
    "    metrics_dict = classification_report(y_test, y_pred, \n",
    "                        target_names=target_names, output_dict=True)\n",
    "    \n",
    "    print(\"Precision:\", metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall:\", metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy:\", metrics_dict[\"accuracy\"])\n",
    "    print()\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_series.name = k\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "metrics_df.to_csv('./MLP3_data/knn_metrics.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Performance with k = 1 *******\n",
      "Precision:  0.7244917639884572\n",
      "Recall:  0.7249801908442532\n",
      "Accuracy:  0.7249801908442532\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 3 *******\n",
      "Precision:  0.753245328754743\n",
      "Recall:  0.7564424168925117\n",
      "Accuracy:  0.7564424168925117\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 5 *******\n",
      "Precision:  0.7653418320274789\n",
      "Recall:  0.7687960100015367\n",
      "Accuracy:  0.7687960100015367\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 10 *******\n",
      "Precision:  0.770844815948166\n",
      "Recall:  0.7738854112577649\n",
      "Accuracy:  0.7738854112577649\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 15 *******\n",
      "Precision:  0.7735824430295486\n",
      "Recall:  0.7770806890803714\n",
      "Accuracy:  0.7770806890803714\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 20 *******\n",
      "Precision:  0.7717879335728262\n",
      "Recall:  0.7749968857567141\n",
      "Accuracy:  0.7749968857567141\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 25 *******\n",
      "Precision:  0.7721984674328923\n",
      "Recall:  0.7757367272235375\n",
      "Accuracy:  0.7757367272235375\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 30 *******\n",
      "Precision:  0.7728088738865446\n",
      "Recall:  0.7760607583514159\n",
      "Accuracy:  0.7760607583514159\n",
      "**************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K-Cross validation (k=10) on the original data\n",
    "# KNN classifier with varying k values\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into k folds \n",
    "\n",
    "k_values = [1, 3, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for k in k_values:\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    train_score = []\n",
    "    test_score = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        \n",
    "        train_score.append(knn.score(X_train, y_train))\n",
    "        test_score.append(knn.score(X_test, y_test))\n",
    "        \n",
    "        #print(f\"k={k}\")\n",
    "        #print(\"training score: \", knn.score(X_train, y_train))\n",
    "        #print(\"testing score: \", knn.score(X_test, y_test))\n",
    "    \n",
    "        # plot a confusion matrix\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "        #print(confusion_mat)\n",
    "    \n",
    "        #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "        results = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        #print(results)\n",
    "\n",
    "        metrics_dict = classification_report(y_test, y_pred, \n",
    "                        target_names=target_names, output_dict=True)\n",
    "\n",
    "        avg_precision = metrics_dict['weighted avg']['precision']\n",
    "        #print('precision (weighted):', avg_precision)\n",
    "        precision.append(avg_precision)\n",
    "    \n",
    "        avg_recall = metrics_dict['weighted avg']['recall']\n",
    "        #print('recall avg (weighted):', avg_recall)\n",
    "        recall.append(avg_recall)\n",
    "    \n",
    "        avg_accuracy = metrics_dict['accuracy']\n",
    "        #print('accuracy: ', avg_accuracy)\n",
    "        accuracy.append(avg_accuracy)\n",
    "        \n",
    "    print('\\n******* Performance with k =', k, '*******')\n",
    "    print('Precision: ', sum(precision)/len(precision))\n",
    "    print('Recall: ', sum(recall)/len(recall))\n",
    "    print('Accuracy: ', sum(accuracy)/len(accuracy))\n",
    "    print('**************************************\\n')\n",
    "    \n",
    "    plt.plot(k, np.mean(test_score), 'bo')\n",
    "    plt.plot(k, np.mean(train_score), 'rx')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 25 is best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bedrooms  bathrooms  sqft_living  sqft_lot  most_recent\n",
      "0         3       1.00         1180      5650         1955\n",
      "1         3       2.25         2570      7242         1991\n",
      "2         2       1.00          770     10000         1933\n",
      "3         4       3.00         1960      5000         1965\n",
      "4         3       2.00         1680      8080         1987\n",
      "   bedrooms  bathrooms  sqft_living  sqft_lot  most_recent\n",
      "0  0.090909    0.12500     0.067170  0.003108     0.478261\n",
      "1  0.090909    0.28125     0.172075  0.004072     0.791304\n",
      "2  0.060606    0.12500     0.036226  0.005743     0.286957\n",
      "3  0.121212    0.37500     0.126038  0.002714     0.565217\n",
      "4  0.090909    0.25000     0.104906  0.004579     0.756522\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "print(data.head())\n",
    "\n",
    "# normalized\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "\n",
    "col = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']\n",
    "normData = min_max.fit_transform(data)\n",
    "\n",
    "normData = pd.DataFrame(normData, columns = col)\n",
    "print(normData.head())\n",
    "\n",
    "X_normalized = normData.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Performance with k = 1 *******\n",
      "Precision:  0.7230262718148852\n",
      "Recall:  0.7236386909732083\n",
      "Accuracy:  0.7236386909732083\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 3 *******\n",
      "Precision:  0.7543211300590373\n",
      "Recall:  0.7574148155540597\n",
      "Accuracy:  0.7574148155540597\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 5 *******\n",
      "Precision:  0.7646939648678418\n",
      "Recall:  0.7681949961487199\n",
      "Accuracy:  0.7681949961487199\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 10 *******\n",
      "Precision:  0.7646665531222592\n",
      "Recall:  0.7680563449277606\n",
      "Accuracy:  0.7680563449277606\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 15 *******\n",
      "Precision:  0.7698558719069393\n",
      "Recall:  0.7734694902451578\n",
      "Accuracy:  0.7734694902451578\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 20 *******\n",
      "Precision:  0.7711632031813526\n",
      "Recall:  0.7745337202466618\n",
      "Accuracy:  0.7745337202466618\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 25 *******\n",
      "Precision:  0.7689489287588592\n",
      "Recall:  0.7727752579946654\n",
      "Accuracy:  0.7727752579946654\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 30 *******\n",
      "Precision:  0.7689507162543996\n",
      "Recall:  0.7727753800128774\n",
      "Accuracy:  0.7727753800128774\n",
      "**************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K-Cross Validation on the normalized data\n",
    "# KNN classifier with varying k values\n",
    "\n",
    "k_values = [1, 3, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for k in k_values:\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    train_score = []\n",
    "    test_score = []\n",
    "    for train_index, test_index in kf.split(X_normalized):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        \n",
    "        train_score.append(knn.score(X_train, y_train))\n",
    "        test_score.append(knn.score(X_test, y_test))\n",
    "        \n",
    "        #print(f\"k={k}\")\n",
    "        #print(\"training score: \", knn.score(X_train, y_train))\n",
    "        #print(\"testing score: \", knn.score(X_test, y_test))\n",
    "    \n",
    "        # plot a confusion matrix\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "        #print(confusion_mat)\n",
    "    \n",
    "        #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "        results = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        #print(results)\n",
    "\n",
    "        metrics_dict = classification_report(y_test, y_pred, \n",
    "                        target_names=target_names, output_dict=True)\n",
    "\n",
    "        avg_precision = metrics_dict['weighted avg']['precision']\n",
    "        #print('precision (weighted):', avg_precision)\n",
    "        precision.append(avg_precision)\n",
    "    \n",
    "        avg_recall = metrics_dict['weighted avg']['recall']\n",
    "        #print('recall avg (weighted):', avg_recall)\n",
    "        recall.append(avg_recall)\n",
    "    \n",
    "        avg_accuracy = metrics_dict['accuracy']\n",
    "        #print('accuracy: ', avg_accuracy)\n",
    "        accuracy.append(avg_accuracy)\n",
    "        \n",
    "    print('\\n******* Performance with k =', k, '*******')\n",
    "    print('Precision: ', sum(precision)/len(precision))\n",
    "    print('Recall: ', sum(recall)/len(recall))\n",
    "    print('Accuracy: ', sum(accuracy)/len(accuracy))\n",
    "    print('**************************************\\n')\n",
    "    \n",
    "    plt.plot(k, np.mean(test_score), 'bo')\n",
    "    plt.plot(k, np.mean(train_score), 'rx')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized vs non-normalized data does not change the model substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1344\n",
      "        Over       0.75      0.61      0.67       818\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.74      0.75      2162\n",
      "weighted avg       0.77      0.78      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.87      0.84      1407\n",
      "        Over       0.71      0.61      0.66       755\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.76      0.74      0.75      2162\n",
      "weighted avg       0.77      0.78      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1379\n",
      "        Over       0.72      0.62      0.66       783\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.74      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1343\n",
      "        Over       0.76      0.62      0.68       818\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.78      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1361\n",
      "        Over       0.75      0.59      0.66       800\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.74      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1381\n",
      "        Over       0.72      0.58      0.64       780\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.73      0.74      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      1393\n",
      "        Over       0.74      0.60      0.66       768\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.74      0.75      2161\n",
      "weighted avg       0.78      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1379\n",
      "        Over       0.74      0.60      0.66       782\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.74      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1329\n",
      "        Over       0.77      0.62      0.68       832\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.78      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1378\n",
      "        Over       0.73      0.58      0.65       783\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.73      0.74      2161\n",
      "weighted avg       0.77      0.77      0.76      2161\n",
      "\n",
      "Avg precision (weighted): 0.7728964691668164\n",
      "Avg recall (weighted): 0.7763846396531567\n",
      "Accuracy: 0.7763846396531567\n"
     ]
    }
   ],
   "source": [
    "# change DataFrame to numpy array\n",
    "XX = X.to_numpy()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 30)\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "for train_index, test_index in kf.split(XX):\n",
    "    X_train, X_test = XX[train_index], XX[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"weighted avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"weighted avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "\n",
    "print(\"Avg precision (weighted):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Avg recall (weighted):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Accuracy:\", accuracy_sum/kf.get_n_splits(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier with K = 25 on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1337\n",
      "        Over       0.75      0.63      0.69       825\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.75      0.76      2162\n",
      "weighted avg       0.78      0.78      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      1368\n",
      "        Over       0.75      0.63      0.68       794\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.75      0.76      2162\n",
      "weighted avg       0.78      0.78      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1370\n",
      "        Over       0.72      0.62      0.67       792\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.74      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      1382\n",
      "        Over       0.74      0.62      0.68       779\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.83      1376\n",
      "        Over       0.72      0.61      0.66       785\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.74      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.84      0.81      1357\n",
      "        Over       0.69      0.60      0.64       804\n",
      "\n",
      "    accuracy                           0.75      2161\n",
      "   macro avg       0.74      0.72      0.73      2161\n",
      "weighted avg       0.75      0.75      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.87      0.83      1376\n",
      "        Over       0.73      0.62      0.67       785\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.87      0.83      1379\n",
      "        Over       0.73      0.62      0.67       782\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.86      0.83      1385\n",
      "        Over       0.72      0.63      0.67       776\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.75      0.75      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.86      0.83      1364\n",
      "        Over       0.73      0.62      0.67       797\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "Average precision: 0.7625259927004021\n",
      "Average recall: 0.7429339650063203\n",
      "Average accuracy: 0.775689981468647\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 25)\n",
    "\n",
    "# 10-fold CV\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # plot a confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    #print(confusion_mat)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision:\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall:\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy:\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "#os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "metrics_df.to_csv('./MLP3_data/best_knn_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data into training data and testing data with 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "\n",
    "# We start with k=10 \n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 10 folds \n",
    "\n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "print (kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.75\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.75\n",
      "Accuracy of GaussianNB classifier on test set: 0.77\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Apply k-cross when k = 10\n",
    "nbclf = GaussianNB()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # for each iteration, get training data and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # train the model using training data\n",
    "    nbclf.fit(X_train, y_train)\n",
    "    \n",
    "    # show how model performs with training data and test data\n",
    "    print('Accuracy of GaussianNB classifier on training set: {:.2f}'\n",
    "         .format(nbclf.score(X_train, y_train)))\n",
    "\n",
    "    print('Accuracy of GaussianNB classifier on test set: {:.2f}'\n",
    "         .format(nbclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB classifier with k-cross = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.76      0.93      0.83      1379\n",
      "        Over       0.79      0.47      0.59       783\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.77      0.70      0.71      2162\n",
      "weighted avg       0.77      0.76      0.75      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.74      0.92      0.82      1338\n",
      "        Over       0.80      0.48      0.60       824\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.77      0.70      0.71      2162\n",
      "weighted avg       0.76      0.76      0.74      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.73      0.92      0.82      1347\n",
      "        Over       0.76      0.45      0.57       815\n",
      "\n",
      "    accuracy                           0.74      2162\n",
      "   macro avg       0.75      0.68      0.69      2162\n",
      "weighted avg       0.75      0.74      0.72      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.74      0.92      0.82      1378\n",
      "        Over       0.75      0.44      0.56       783\n",
      "\n",
      "    accuracy                           0.74      2161\n",
      "   macro avg       0.75      0.68      0.69      2161\n",
      "weighted avg       0.75      0.74      0.73      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.77      0.93      0.84      1407\n",
      "        Over       0.78      0.47      0.58       754\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.77      0.70      0.71      2161\n",
      "weighted avg       0.77      0.77      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.75      0.93      0.83      1364\n",
      "        Over       0.79      0.48      0.60       797\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.77      0.70      0.71      2161\n",
      "weighted avg       0.77      0.76      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.76      0.92      0.84      1389\n",
      "        Over       0.78      0.49      0.60       772\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.77      0.71      0.72      2161\n",
      "weighted avg       0.77      0.77      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.74      0.94      0.83      1350\n",
      "        Over       0.82      0.44      0.57       811\n",
      "\n",
      "    accuracy                           0.75      2161\n",
      "   macro avg       0.78      0.69      0.70      2161\n",
      "weighted avg       0.77      0.75      0.73      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.76      0.92      0.83      1408\n",
      "        Over       0.76      0.46      0.57       753\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.76      0.69      0.70      2161\n",
      "weighted avg       0.76      0.76      0.74      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.73      0.93      0.82      1334\n",
      "        Over       0.79      0.44      0.57       827\n",
      "\n",
      "    accuracy                           0.74      2161\n",
      "   macro avg       0.76      0.68      0.69      2161\n",
      "weighted avg       0.75      0.74      0.72      2161\n",
      "\n",
      "Avg precision: 0.7612161323968912\n",
      "Avg recall: 0.7559342494416835\n",
      "Accuracy: 0.7559342494416835\n"
     ]
    }
   ],
   "source": [
    "# Model performance using k-cross\n",
    "nbclf2 = GaussianNB()\n",
    "\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # for each iteration, get training data and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # train the model using training data\n",
    "    nbclf2.fit(X_train, y_train)\n",
    "    \n",
    "    # predict y values using test data\n",
    "    y_pred = nbclf2.predict(X_test)\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    #print(confusion_mat)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Since we can retrieve a dictionary of metrics and access the values using dictionary,\n",
    "    # now we can sum of the results of each iteration and get the average\n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    #print(result_metrics_dict)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"weighted avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"weighted avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"weighted avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"weighted avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Avg precision:\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Avg recall:\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Accuracy:\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "metrics_df.to_csv('./MLP3_data/nb_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8228294890777875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)\n",
    "\n",
    "y_score = nbclf2.predict_proba(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_score[:,1])\n",
    "\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('Accuracy = ', roc_auc)\n",
    "\n",
    "# Plotting\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=('Accuracy = %0.2f'%roc_auc))\n",
    "plt.legend(loc='lower right', prop={'size':8})\n",
    "plt.plot([0,1],[0,1], color='lightgrey', linestyle='--')\n",
    "plt.xlim([-0.05,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
