{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and training-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "0           0  7129300520  20141013T000000  221900.0         3       1.00   \n",
      "1           1  6414100192  20141209T000000  538000.0         3       2.25   \n",
      "2           2  5631500400  20150225T000000  180000.0         2       1.00   \n",
      "3           3  2487200875  20141209T000000  604000.0         4       3.00   \n",
      "4           4  1954400510  20150218T000000  510000.0         3       2.00   \n",
      "\n",
      "   sqft_living  sqft_lot  floors  waterfront  ...  yr_built  yr_renovated  \\\n",
      "0         1180      5650     1.0           0  ...      1955             0   \n",
      "1         2570      7242     2.0           0  ...      1951          1991   \n",
      "2          770     10000     1.0           0  ...      1933             0   \n",
      "3         1960      5000     1.0           0  ...      1965             0   \n",
      "4         1680      8080     1.0           0  ...      1987             0   \n",
      "\n",
      "   zipcode      lat     long  sqft_living15  sqft_lot15   date_time  \\\n",
      "0    98178  47.5112 -122.257           1340        5650  2014-10-13   \n",
      "1    98125  47.7210 -122.319           1690        7639  2014-12-09   \n",
      "2    98028  47.7379 -122.233           2720        8062  2015-02-25   \n",
      "3    98136  47.5208 -122.393           1360        5000  2014-12-09   \n",
      "4    98074  47.6168 -122.045           1800        7503  2015-02-18   \n",
      "\n",
      "   most_recent  price_range  \n",
      "0         1955            0  \n",
      "1         1991            0  \n",
      "2         1933            0  \n",
      "3         1965            1  \n",
      "4         1987            0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "       Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "21608       21608   263000018  20140521T000000  360000.0         3       2.50   \n",
      "21609       21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
      "21610       21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
      "21611       21611   291310100  20150116T000000  400000.0         3       2.50   \n",
      "21612       21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
      "\n",
      "       sqft_living  sqft_lot  floors  waterfront  ...  yr_built  yr_renovated  \\\n",
      "21608         1530      1131     3.0           0  ...      2009             0   \n",
      "21609         2310      5813     2.0           0  ...      2014             0   \n",
      "21610         1020      1350     2.0           0  ...      2009             0   \n",
      "21611         1600      2388     2.0           0  ...      2004             0   \n",
      "21612         1020      1076     2.0           0  ...      2008             0   \n",
      "\n",
      "       zipcode      lat     long  sqft_living15  sqft_lot15   date_time  \\\n",
      "21608    98103  47.6993 -122.346           1530        1509  2014-05-21   \n",
      "21609    98146  47.5107 -122.362           1830        7200  2015-02-23   \n",
      "21610    98144  47.5944 -122.299           1020        2007  2014-06-23   \n",
      "21611    98027  47.5345 -122.069           1410        1287  2015-01-16   \n",
      "21612    98144  47.5941 -122.299           1020        1357  2014-10-15   \n",
      "\n",
      "       most_recent  price_range  \n",
      "21608         2009            0  \n",
      "21609         2014            0  \n",
      "21610         2009            0  \n",
      "21611         2004            0  \n",
      "21612         2008            0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# read data from CSV file to dataframe\n",
    "df = pd.read_csv(r\"./input/kc_sales_cleaned.csv\")\n",
    "\n",
    "# make sure you understand the type of the object\n",
    "print(type(df))\n",
    "\n",
    "# check the top five and the botoom five data tuples\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We use bedrooms, bathrooms, sqft_living, sqft_lot, and 'most_recent' attributes\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Under', 'Over']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.83      0.83      1410\n",
      "        Over       0.68      0.67      0.68       752\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.75      0.75      0.75      2162\n",
      "weighted avg       0.78      0.78      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.83      0.82      1358\n",
      "        Over       0.69      0.66      0.68       804\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.75      0.75      0.75      2162\n",
      "weighted avg       0.76      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.81      0.81      1360\n",
      "        Over       0.68      0.67      0.68       802\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.74      0.74      0.74      2162\n",
      "weighted avg       0.76      0.76      0.76      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.85      0.82      1353\n",
      "        Over       0.71      0.64      0.67       808\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.82      0.81      1349\n",
      "        Over       0.69      0.66      0.67       812\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.74      0.74      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.83      0.81      1367\n",
      "        Over       0.69      0.64      0.66       794\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.74      0.74      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.84      0.83      1358\n",
      "        Over       0.71      0.68      0.70       803\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.76      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1385\n",
      "        Over       0.68      0.67      0.67       776\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.83      0.82      1386\n",
      "        Over       0.69      0.67      0.68       775\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.82      1368\n",
      "        Over       0.71      0.61      0.66       793\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.73      0.74      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "Average precision (ENTROPY): 0.7503331495880862\n",
      "Average recall (ENTROPY): 0.744359754019261\n",
      "Average accuracy (ENTROPY): 0.7675936124408775\n"
     ]
    }
   ],
   "source": [
    "# Entropy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "\n",
    "# decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy', random_state=42)\n",
    "\n",
    "# k-fold cross validation (k=10)\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 2 folds\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "# each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    # Store metrics data for later analysis\n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision (ENTROPY):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (ENTROPY):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (ENTROPY):\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "metrics_df.to_csv('./MLP3_data/entropy_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.82      0.82      1398\n",
      "        Over       0.67      0.65      0.66       764\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.74      0.74      0.74      2162\n",
      "weighted avg       0.76      0.76      0.76      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.85      0.83      1409\n",
      "        Over       0.69      0.63      0.66       753\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.75      0.74      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.82      0.82      1353\n",
      "        Over       0.69      0.69      0.69       809\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.75      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.84      0.82      1333\n",
      "        Over       0.72      0.67      0.69       828\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.75      0.76      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.85      0.83      1372\n",
      "        Over       0.71      0.66      0.69       789\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.76      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.87      0.83      1376\n",
      "        Over       0.73      0.63      0.67       785\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.75      0.75      2161\n",
      "weighted avg       0.78      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.82      1346\n",
      "        Over       0.72      0.62      0.67       815\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.74      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.84      0.81      1373\n",
      "        Over       0.69      0.61      0.64       788\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.72      0.73      2161\n",
      "weighted avg       0.75      0.76      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.82      0.81      1364\n",
      "        Over       0.68      0.65      0.66       797\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.74      0.73      0.73      2161\n",
      "weighted avg       0.75      0.76      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.84      0.82      1370\n",
      "        Over       0.70      0.64      0.67       791\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.75      0.74      0.74      2161\n",
      "weighted avg       0.76      0.77      0.77      2161\n",
      "\n",
      "Average precision (GINI): 0.751704259154597\n",
      "Average recall (GINI): 0.7423455464424358\n",
      "Average accuracy (GINI): 0.7683800926439219\n"
     ]
    }
   ],
   "source": [
    "# Gini\n",
    "\n",
    "# Construct a decision tree using gini index\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='gini', random_state=42)\n",
    "\n",
    "# 10-fold CV\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # read to file\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision (GINI):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (GINI):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (GINI):\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "metrics_df.to_csv('./MLP3_data/gini_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance information for gini classifier on max depth =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.84      0.83      2726\n",
      "        Over       0.72      0.70      0.71      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.77      0.77      0.77      4323\n",
      "weighted avg       0.79      0.79      0.79      4323\n",
      "\n",
      "Precision (GINI): 0.7715737156931738\n",
      "Recall (GINI): 0.7694788145968849\n",
      "Accuracy (GINI): 0.7869535045107564\n",
      "\n",
      "Performance information for entropy classifier on max depth =  3 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.84      0.81      0.83      2726\n",
      "        Over       0.69      0.74      0.72      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.78      0.77      4323\n",
      "weighted avg       0.79      0.78      0.79      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7688380414160795\n",
      "Recall (ENTROPY): 0.7757599653789593\n",
      "Accuracy (ENTROPY): 0.7844089752486699\n",
      "\n",
      "Performance information for gini classifier on max depth =  4 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      2726\n",
      "        Over       0.78      0.59      0.67      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (GINI): 0.784653704893585\n",
      "Recall (GINI): 0.7472834014253615\n",
      "Accuracy (GINI): 0.7874161461947722\n",
      "\n",
      "Performance information for entropy classifier on max depth =  4 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      2726\n",
      "        Over       0.78      0.60      0.68      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.79      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7861806827314528\n",
      "Recall (ENTROPY): 0.7494750106927378\n",
      "Accuracy (ENTROPY): 0.7890353920888272\n",
      "\n",
      "Performance information for gini classifier on max depth =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      2726\n",
      "        Over       0.76      0.63      0.69      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.75      0.76      4323\n",
      "weighted avg       0.79      0.79      0.78      4323\n",
      "\n",
      "Precision (GINI): 0.7792928075916197\n",
      "Recall (GINI): 0.754522649079276\n",
      "Accuracy (GINI): 0.7878787878787878\n",
      "\n",
      "Performance information for entropy classifier on max depth =  5 :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.87      0.84      2726\n",
      "        Over       0.74      0.67      0.70      1597\n",
      "\n",
      "    accuracy                           0.79      4323\n",
      "   macro avg       0.78      0.77      0.77      4323\n",
      "weighted avg       0.79      0.79      0.79      4323\n",
      "\n",
      "Precision (ENTROPY): 0.7800784522996014\n",
      "Recall (ENTROPY): 0.7664360358357173\n",
      "Accuracy (ENTROPY): 0.7922738838769373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change max depth and observe results \n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)\n",
    "\n",
    "gini_metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "entropy_metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for i in [3,4,5]:\n",
    "    tree_clf_gini = DecisionTreeClassifier(max_depth= i, criterion=\"gini\", random_state=42)\n",
    "    tree_clf_entropy = DecisionTreeClassifier(max_depth = i, criterion=\"entropy\", random_state=42)\n",
    "    \n",
    "    tree_clf_gini.fit(X_train, y_train)\n",
    "    tree_clf_entropy.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_gini = tree_clf_gini.predict(X_test)\n",
    "    \n",
    "    y_pred_entropy = tree_clf_entropy.predict(X_test)    \n",
    "    \n",
    "    print(\"Performance information for gini classifier on max depth = \", i, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_gini, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict_gini = classification_report(y_test, y_pred_gini, target_names=target_names, output_dict=True)\n",
    "    print(\"Precision (GINI):\", result_metrics_dict_gini[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall (GINI):\", result_metrics_dict_gini[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy (GINI):\", result_metrics_dict_gini[\"accuracy\"])\n",
    "    print()\n",
    "    \n",
    "    print(\"Performance information for entropy classifier on max depth = \", i, \":\")\n",
    "    print()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_entropy, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict_entropy = classification_report(y_test, y_pred_entropy, target_names=target_names, output_dict=True)\n",
    "    print(\"Precision (ENTROPY):\", result_metrics_dict_entropy[\"macro avg\"][\"precision\"])\n",
    "    print(\"Recall (ENTROPY):\", result_metrics_dict_entropy[\"macro avg\"][\"recall\"])\n",
    "    print(\"Accuracy (ENTROPY):\", result_metrics_dict_entropy[\"accuracy\"])\n",
    "    print()\n",
    "    \n",
    "    gini_metrics_list = []\n",
    "    gini_metrics_list.append(result_metrics_dict_gini[\"macro avg\"][\"precision\"])\n",
    "    gini_metrics_list.append(result_metrics_dict_gini[\"macro avg\"][\"recall\"])\n",
    "    gini_metrics_list.append(result_metrics_dict_gini[\"accuracy\"])\n",
    "    \n",
    "    gini_metrics_series = pd.Series(gini_metrics_list, index=gini_metrics_df.columns)\n",
    "    gini_metrics_series.name = i\n",
    "    gini_metrics_df = gini_metrics_df.append(gini_metrics_series)\n",
    "    \n",
    "    entropy_metrics_list = []\n",
    "    entropy_metrics_list.append(result_metrics_dict_entropy[\"macro avg\"][\"precision\"])\n",
    "    entropy_metrics_list.append(result_metrics_dict_entropy[\"macro avg\"][\"recall\"])\n",
    "    entropy_metrics_list.append(result_metrics_dict_entropy[\"accuracy\"])\n",
    "    \n",
    "    entropy_metrics_series = pd.Series(entropy_metrics_list, index=entropy_metrics_df.columns)\n",
    "    entropy_metrics_series.name = i\n",
    "    entropy_metrics_df = entropy_metrics_df.append(entropy_metrics_series)\n",
    "      \n",
    "gini_metrics_df.to_csv('./MLP3_data/gini_depth_metrics.csv')\n",
    "entropy_metrics_df.to_csv('./MLP3_data/entropy_depth_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for K =  3 \n",
      "\n",
      "Average precision (GINI): 0.7842357101370668\n",
      "Average recall (GINI): 0.7543143091297481\n",
      "Average accuracy (GINI): 0.7910518779052375 \n",
      "\n",
      "Average precision (ENTROPY): 0.7784626818032855\n",
      "Average recall (ENTROPY): 0.7620881182030738\n",
      "Average accuracy (ENTROPY): 0.7905891077809986 \n",
      "\n",
      "Results for K =  5 \n",
      "\n",
      "Average precision (GINI): 0.7813917596216193\n",
      "Average recall (GINI): 0.7533246051759319\n",
      "Average accuracy (GINI): 0.7894326195356607 \n",
      "\n",
      "Average precision (ENTROPY): 0.7783766143806481\n",
      "Average recall (ENTROPY): 0.7572549110496708\n",
      "Average accuracy (ENTROPY): 0.7886458717686132 \n",
      "\n",
      "Results for K =  7 \n",
      "\n",
      "Average precision (GINI): 0.7791384270824023\n",
      "Average recall (GINI): 0.7519252266027997\n",
      "Average accuracy (GINI): 0.7874887874151459 \n",
      "\n",
      "Average precision (ENTROPY): 0.7776391119890638\n",
      "Average recall (ENTROPY): 0.7586305424816775\n",
      "Average accuracy (ENTROPY): 0.7888305360916352 \n",
      "\n",
      "Results for K =  10 \n",
      "\n",
      "Average precision (GINI): 0.7787629914134627\n",
      "Average recall (GINI): 0.7540267001654275\n",
      "Average accuracy (GINI): 0.7880897424317468 \n",
      "\n",
      "Average precision (ENTROPY): 0.777115373088708\n",
      "Average recall (ENTROPY): 0.7562798880279982\n",
      "Average accuracy (ENTROPY): 0.7874883403159447 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change k value and observe results\n",
    "\n",
    "for k in [3,5,7,10]:\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    precision_sum_gini = recall_sum_gini = accuracy_sum_gini = 0\n",
    "    precision_sum_entropy = recall_sum_entropy = accuracy_sum_entropy = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        tree_clf_gini = DecisionTreeClassifier(max_depth= i, criterion=\"gini\", random_state=42)\n",
    "        tree_clf_entropy = DecisionTreeClassifier(max_depth = i, criterion=\"entropy\", random_state=42)\n",
    "    \n",
    "        tree_clf_gini.fit(X_train, y_train)\n",
    "        tree_clf_entropy.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred_gini = tree_clf_gini.predict(X_test)\n",
    "        y_pred_entropy = tree_clf_entropy.predict(X_test)  \n",
    "        \n",
    "    \n",
    "        result_metrics_dict_gini = classification_report(y_test, y_pred_gini, target_names=target_names, output_dict=True)\n",
    "        result_metrics_dict_entropy = classification_report(y_test, y_pred_entropy, target_names=target_names, output_dict=True)\n",
    "    \n",
    "        precision_sum_gini += result_metrics_dict_gini[\"macro avg\"][\"precision\"]\n",
    "        recall_sum_gini += result_metrics_dict_gini[\"macro avg\"][\"recall\"]\n",
    "        accuracy_sum_gini += result_metrics_dict_gini[\"accuracy\"]\n",
    "        \n",
    "        precision_sum_entropy += result_metrics_dict_entropy[\"macro avg\"][\"precision\"]\n",
    "        recall_sum_entropy += result_metrics_dict_entropy[\"macro avg\"][\"recall\"]\n",
    "        accuracy_sum_entropy += result_metrics_dict_entropy[\"accuracy\"]\n",
    "        \n",
    "    print(\"Results for K = \", k, \"\\n\")\n",
    "    \n",
    "    print(\"Average precision (GINI):\", precision_sum_gini/kf.get_n_splits(X))\n",
    "    print(\"Average recall (GINI):\", recall_sum_gini/kf.get_n_splits(X))\n",
    "    print(\"Average accuracy (GINI):\", accuracy_sum_gini/kf.get_n_splits(X), \"\\n\")\n",
    "    \n",
    "    print(\"Average precision (ENTROPY):\", precision_sum_entropy/kf.get_n_splits(X))\n",
    "    print(\"Average recall (ENTROPY):\", recall_sum_entropy/kf.get_n_splits(X))\n",
    "    print(\"Average accuracy (ENTROPY):\", accuracy_sum_entropy/kf.get_n_splits(X), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the gini and entropy classifiers was actually fairly similar on the same testing set, and the best performing depth appeared to be max depth = 5. Changing the value of K didn't change the results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot important features\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT classifier on training set: 1.00\n",
      "Accuracy of DT classifier on test set: 0.74\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Find the important features of the entropy DT model\n",
    "clf1 = DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(clf1.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(clf1.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=60)\n",
    "\n",
    "# call the function above\n",
    "plot_feature_importances(clf1, X.columns.values.tolist())\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(tree_clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider entropy Decision Tree with max depth = 5 and K = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree with max depth = 5 and K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.86      0.84      1337\n",
      "        Over       0.75      0.68      0.71       825\n",
      "\n",
      "    accuracy                           0.79      2162\n",
      "   macro avg       0.78      0.77      0.77      2162\n",
      "weighted avg       0.79      0.79      0.79      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      1368\n",
      "        Over       0.78      0.59      0.67       794\n",
      "\n",
      "    accuracy                           0.79      2162\n",
      "   macro avg       0.79      0.74      0.76      2162\n",
      "weighted avg       0.79      0.79      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.90      0.84      1370\n",
      "        Over       0.77      0.58      0.66       792\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.78      0.74      0.75      2162\n",
      "weighted avg       0.78      0.78      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.86      0.85      1382\n",
      "        Over       0.74      0.69      0.71       779\n",
      "\n",
      "    accuracy                           0.80      2161\n",
      "   macro avg       0.78      0.77      0.78      2161\n",
      "weighted avg       0.80      0.80      0.80      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      1376\n",
      "        Over       0.74      0.62      0.68       785\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.85      0.83      1357\n",
      "        Over       0.73      0.68      0.71       804\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.77      0.77      2161\n",
      "weighted avg       0.79      0.79      0.79      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.82      0.85      0.83      1376\n",
      "        Over       0.72      0.67      0.69       785\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.76      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.85      0.84      1379\n",
      "        Over       0.73      0.69      0.71       782\n",
      "\n",
      "    accuracy                           0.80      2161\n",
      "   macro avg       0.78      0.77      0.78      2161\n",
      "weighted avg       0.79      0.80      0.79      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.83      0.85      0.84      1385\n",
      "        Over       0.72      0.68      0.70       776\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.77      0.77      2161\n",
      "weighted avg       0.79      0.79      0.79      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.86      0.83      1364\n",
      "        Over       0.73      0.66      0.69       797\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.76      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "Average precision (ENTROPY): 0.7770743854655505\n",
      "Average recall (ENTROPY): 0.7603184793752781\n",
      "Average accuracy (ENTROPY): 0.7888772714177534\n"
     ]
    }
   ],
   "source": [
    "# Entropy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "\n",
    "# decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=42)\n",
    "\n",
    "# k-fold cross validation (k=10)\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 2 folds\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "# each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    # Store metrics data for later analysis\n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision (ENTROPY):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall (ENTROPY):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy (ENTROPY):\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "#os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "#metrics_df.to_csv('./MLP3_data/entropy_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: kNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use bedrooms, bathrooms, sqft_living, sqft_lot, and 'most_recent' attributes\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data 80% for training, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1\n",
      "[[2179  547]\n",
      " [ 635  962]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.77      0.80      0.79      2726\n",
      "        Over       0.64      0.60      0.62      1597\n",
      "\n",
      "    accuracy                           0.73      4323\n",
      "   macro avg       0.71      0.70      0.70      4323\n",
      "weighted avg       0.72      0.73      0.72      4323\n",
      "\n",
      "k=5\n",
      "[[2356  370]\n",
      " [ 617  980]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.83      2726\n",
      "        Over       0.73      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=15\n",
      "[[2364  362]\n",
      " [ 615  982]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      2726\n",
      "        Over       0.73      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.76      0.74      0.75      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=20\n",
      "[[2422  304]\n",
      " [ 675  922]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.89      0.83      2726\n",
      "        Over       0.75      0.58      0.65      1597\n",
      "\n",
      "    accuracy                           0.77      4323\n",
      "   macro avg       0.77      0.73      0.74      4323\n",
      "weighted avg       0.77      0.77      0.77      4323\n",
      "\n",
      "k=25\n",
      "[[2389  337]\n",
      " [ 619  978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      2726\n",
      "        Over       0.74      0.61      0.67      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.78      0.78      0.77      4323\n",
      "\n",
      "k=30\n",
      "[[2417  309]\n",
      " [ 658  939]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.89      0.83      2726\n",
      "        Over       0.75      0.59      0.66      1597\n",
      "\n",
      "    accuracy                           0.78      4323\n",
      "   macro avg       0.77      0.74      0.75      4323\n",
      "weighted avg       0.77      0.78      0.77      4323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the best k value\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_value = [1, 5, 15, 20, 25, 30]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for k in k_value:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"k={k}\")\n",
    "    \n",
    "    # plot a confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    # Print classification report\n",
    "    target_names = ['Under', 'Over']\n",
    "   \n",
    "    #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    result_metrics = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(result_metrics)\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_series.name = k\n",
    "    metrics_df = metrics_df.append(metrics_series)\n",
    "\n",
    "metrics_df.to_csv('./MLP3_data/knn_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Performance with k = 1 *******\n",
      "Precision:  0.7223142849111975\n",
      "Recall:  0.7227132213668531\n",
      "Accuracy:  0.7227132213668531\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 3 *******\n",
      "Precision:  0.7511881853068534\n",
      "Recall:  0.7540831468060191\n",
      "Accuracy:  0.7540831468060191\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 5 *******\n",
      "Precision:  0.7602495682267002\n",
      "Recall:  0.7638456120260123\n",
      "Accuracy:  0.7638456120260123\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 10 *******\n",
      "Precision:  0.7648813509063372\n",
      "Recall:  0.7683338207639805\n",
      "Accuracy:  0.7683338207639805\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 15 *******\n",
      "Precision:  0.7684567991363468\n",
      "Recall:  0.7722666603987838\n",
      "Accuracy:  0.7722666603987838\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 20 *******\n",
      "Precision:  0.769176734786725\n",
      "Recall:  0.7728220102359152\n",
      "Accuracy:  0.7728220102359152\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 25 *******\n",
      "Precision:  0.7699248850820828\n",
      "Recall:  0.773747550484393\n",
      "Accuracy:  0.773747550484393\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 30 *******\n",
      "Precision:  0.7692727312506377\n",
      "Recall:  0.7729144422425508\n",
      "Accuracy:  0.7729144422425508\n",
      "**************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K-Cross validation (k=3) on the original data\n",
    "# KNN classifier with varying k values\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=None, shuffle=True) # Define the split - into k folds \n",
    "\n",
    "k_values = [1, 3, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for k in k_values:\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    train_score = []\n",
    "    test_score = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        \n",
    "        train_score.append(knn.score(X_train, y_train))\n",
    "        test_score.append(knn.score(X_test, y_test))\n",
    "        \n",
    "        #print(f\"k={k}\")\n",
    "        #print(\"training score: \", knn.score(X_train, y_train))\n",
    "        #print(\"testing score: \", knn.score(X_test, y_test))\n",
    "    \n",
    "        # plot a confusion matrix\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "        #print(confusion_mat)\n",
    "    \n",
    "        #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "        results = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        #print(results)\n",
    "\n",
    "        metrics_dict = classification_report(y_test, y_pred, \n",
    "                        target_names=target_names, output_dict=True)\n",
    "\n",
    "        avg_precision = metrics_dict['weighted avg']['precision']\n",
    "        #print('precision (weighted):', avg_precision)\n",
    "        precision.append(avg_precision)\n",
    "    \n",
    "        avg_recall = metrics_dict['weighted avg']['recall']\n",
    "        #print('recall avg (weighted):', avg_recall)\n",
    "        recall.append(avg_recall)\n",
    "    \n",
    "        avg_accuracy = metrics_dict['accuracy']\n",
    "        #print('accuracy: ', avg_accuracy)\n",
    "        accuracy.append(avg_accuracy)\n",
    "        \n",
    "    print('\\n******* Performance with k =', k, '*******')\n",
    "    print('Precision: ', sum(precision)/len(precision))\n",
    "    print('Recall: ', sum(recall)/len(recall))\n",
    "    print('Accuracy: ', sum(accuracy)/len(accuracy))\n",
    "    print('**************************************\\n')\n",
    "    \n",
    "    plt.plot(k, np.mean(test_score), 'bo')\n",
    "    plt.plot(k, np.mean(train_score), 'rx')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 30 is best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bedrooms  bathrooms  sqft_living  sqft_lot  most_recent\n",
      "0         3       1.00         1180      5650         1955\n",
      "1         3       2.25         2570      7242         1991\n",
      "2         2       1.00          770     10000         1933\n",
      "3         4       3.00         1960      5000         1965\n",
      "4         3       2.00         1680      8080         1987\n",
      "   bedrooms  bathrooms  sqft_living  sqft_lot  most_recent\n",
      "0  0.090909    0.12500     0.067170  0.003108     0.478261\n",
      "1  0.090909    0.28125     0.172075  0.004072     0.791304\n",
      "2  0.060606    0.12500     0.036226  0.005743     0.286957\n",
      "3  0.121212    0.37500     0.126038  0.002714     0.565217\n",
      "4  0.090909    0.25000     0.104906  0.004579     0.756522\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "print(data.head())\n",
    "\n",
    "# normalized\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "\n",
    "col = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']\n",
    "normData = min_max.fit_transform(data)\n",
    "\n",
    "normData = pd.DataFrame(normData, columns = col)\n",
    "print(normData.head())\n",
    "\n",
    "X_normalized = normData.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Performance with k = 1 *******\n",
      "Precision:  0.7192233709132082\n",
      "Recall:  0.719798174170851\n",
      "Accuracy:  0.719798174170851\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 3 *******\n",
      "Precision:  0.7497055512989208\n",
      "Recall:  0.7528339050849869\n",
      "Accuracy:  0.7528339050849869\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 5 *******\n",
      "Precision:  0.7635897619206857\n",
      "Recall:  0.7671307918352605\n",
      "Accuracy:  0.7671307918352605\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 10 *******\n",
      "Precision:  0.7668028331983016\n",
      "Recall:  0.7701847920867465\n",
      "Accuracy:  0.7701847920867465\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 15 *******\n",
      "Precision:  0.7709354430588234\n",
      "Recall:  0.7747187897642904\n",
      "Accuracy:  0.7747187897642904\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 20 *******\n",
      "Precision:  0.7681464613833189\n",
      "Recall:  0.7718040122927569\n",
      "Accuracy:  0.7718040122927569\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 25 *******\n",
      "Precision:  0.7720678326921595\n",
      "Recall:  0.7757831032519394\n",
      "Accuracy:  0.7757831032519394\n",
      "**************************************\n",
      "\n",
      "\n",
      "******* Performance with k = 30 *******\n",
      "Precision:  0.7706789236262731\n",
      "Recall:  0.7743489011874685\n",
      "Accuracy:  0.7743489011874685\n",
      "**************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K-Cross Validation on the normalized data\n",
    "# KNN classifier with varying k values\n",
    "\n",
    "k_values = [1, 3, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for k in k_values:\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    train_score = []\n",
    "    test_score = []\n",
    "    for train_index, test_index in kf.split(X_normalized):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        \n",
    "        train_score.append(knn.score(X_train, y_train))\n",
    "        test_score.append(knn.score(X_test, y_test))\n",
    "        \n",
    "        #print(f\"k={k}\")\n",
    "        #print(\"training score: \", knn.score(X_train, y_train))\n",
    "        #print(\"testing score: \", knn.score(X_test, y_test))\n",
    "    \n",
    "        # plot a confusion matrix\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "        #print(confusion_mat)\n",
    "    \n",
    "        #classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "        results = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        #print(results)\n",
    "\n",
    "        metrics_dict = classification_report(y_test, y_pred, \n",
    "                        target_names=target_names, output_dict=True)\n",
    "\n",
    "        avg_precision = metrics_dict['weighted avg']['precision']\n",
    "        #print('precision (weighted):', avg_precision)\n",
    "        precision.append(avg_precision)\n",
    "    \n",
    "        avg_recall = metrics_dict['weighted avg']['recall']\n",
    "        #print('recall avg (weighted):', avg_recall)\n",
    "        recall.append(avg_recall)\n",
    "    \n",
    "        avg_accuracy = metrics_dict['accuracy']\n",
    "        #print('accuracy: ', avg_accuracy)\n",
    "        accuracy.append(avg_accuracy)\n",
    "        \n",
    "    print('\\n******* Performance with k =', k, '*******')\n",
    "    print('Precision: ', sum(precision)/len(precision))\n",
    "    print('Recall: ', sum(recall)/len(recall))\n",
    "    print('Accuracy: ', sum(accuracy)/len(accuracy))\n",
    "    print('**************************************\\n')\n",
    "    \n",
    "    plt.plot(k, np.mean(test_score), 'bo')\n",
    "    plt.plot(k, np.mean(train_score), 'rx')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized vs non-normalized data does not change much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.83      1379\n",
      "        Over       0.74      0.60      0.66       783\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.74      0.75      2162\n",
      "weighted avg       0.77      0.78      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1383\n",
      "        Over       0.73      0.60      0.66       779\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.74      0.74      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.88      0.83      1354\n",
      "        Over       0.75      0.59      0.66       808\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.77      0.74      0.75      2162\n",
      "weighted avg       0.77      0.77      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1368\n",
      "        Over       0.73      0.61      0.67       793\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.78      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.87      0.82      1366\n",
      "        Over       0.72      0.57      0.64       795\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.75      0.72      0.73      2161\n",
      "weighted avg       0.76      0.76      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.89      0.84      1390\n",
      "        Over       0.75      0.61      0.67       771\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.75      0.76      2161\n",
      "weighted avg       0.78      0.79      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.80      0.88      0.84      1373\n",
      "        Over       0.75      0.62      0.68       788\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.78      0.75      0.76      2161\n",
      "weighted avg       0.78      0.79      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1376\n",
      "        Over       0.73      0.60      0.66       785\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.74      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.87      0.83      1361\n",
      "        Over       0.74      0.60      0.66       800\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.87      0.83      1344\n",
      "        Over       0.75      0.60      0.67       817\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.75      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "Avg precision (weighted): 0.7722794030453511\n",
      "Avg recall (weighted): 0.7758756588604396\n",
      "Accuracy: 0.7758756588604396\n"
     ]
    }
   ],
   "source": [
    "# change DataFrame to numpy array\n",
    "XX = X.to_numpy()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 30)\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "for train_index, test_index in kf.split(XX):\n",
    "    X_train, X_test = XX[train_index], XX[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"weighted avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"weighted avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "\n",
    "print(\"Avg precision (weighted):\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Avg recall (weighted):\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Accuracy:\", accuracy_sum/kf.get_n_splits(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier with K = 30 on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.88      0.84      1410\n",
      "        Over       0.74      0.61      0.67       752\n",
      "\n",
      "    accuracy                           0.79      2162\n",
      "   macro avg       0.77      0.75      0.76      2162\n",
      "weighted avg       0.78      0.79      0.78      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.86      0.82      1358\n",
      "        Over       0.73      0.61      0.66       804\n",
      "\n",
      "    accuracy                           0.77      2162\n",
      "   macro avg       0.76      0.74      0.74      2162\n",
      "weighted avg       0.76      0.77      0.76      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.83      1360\n",
      "        Over       0.75      0.60      0.67       802\n",
      "\n",
      "    accuracy                           0.78      2162\n",
      "   macro avg       0.77      0.74      0.75      2162\n",
      "weighted avg       0.77      0.78      0.77      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.88      0.83      1353\n",
      "        Over       0.74      0.60      0.66       808\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.74      0.74      2161\n",
      "weighted avg       0.77      0.77      0.77      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.87      0.82      1349\n",
      "        Over       0.74      0.60      0.66       812\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.73      0.74      2161\n",
      "weighted avg       0.76      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.89      0.84      1367\n",
      "        Over       0.76      0.60      0.67       794\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.77      0.74      0.75      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.79      0.88      0.84      1358\n",
      "        Over       0.76      0.61      0.68       803\n",
      "\n",
      "    accuracy                           0.78      2161\n",
      "   macro avg       0.78      0.75      0.76      2161\n",
      "weighted avg       0.78      0.78      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.81      0.87      0.84      1385\n",
      "        Over       0.74      0.63      0.68       776\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.77      0.75      0.76      2161\n",
      "weighted avg       0.78      0.79      0.78      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.87      0.82      1386\n",
      "        Over       0.71      0.57      0.63       775\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.75      0.72      0.73      2161\n",
      "weighted avg       0.76      0.76      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.78      0.89      0.83      1368\n",
      "        Over       0.74      0.58      0.65       793\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.76      0.73      0.74      2161\n",
      "weighted avg       0.77      0.77      0.76      2161\n",
      "\n",
      "Average precision: 0.7650128226015707\n",
      "Average recall: 0.7388695429144937\n",
      "Average accuracy: 0.7757365559936662\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 30)\n",
    "\n",
    "# 10-fold CV\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) \n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "# DataFrame for storing validation metrics\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # plot a confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    #print(confusion_mat)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"macro avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"macro avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"macro avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Average precision:\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Average recall:\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Average accuracy:\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "#os.makedirs('./MLP3_data', exist_ok=True)  \n",
    "#metrics_df.to_csv('./MLP3_data/gini_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'most_recent']]\n",
    "y = df['price_range']\n",
    "\n",
    "# split the data into training data and testing data with 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "\n",
    "# We start with k=10 \n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) # Define the split - into 10 folds \n",
    "\n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "print (kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.77\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.74\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.74\n",
      "Accuracy of GaussianNB classifier on training set: 0.75\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.77\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.75\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n",
      "Accuracy of GaussianNB classifier on training set: 0.76\n",
      "Accuracy of GaussianNB classifier on test set: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Apply k-cross when k = 10\n",
    "nbclf = GaussianNB()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # for each iteration, get training data and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # train the model using training data\n",
    "    nbclf.fit(X_train, y_train)\n",
    "    \n",
    "    # show how model performs with training data and test data\n",
    "    print('Accuracy of GaussianNB classifier on training set: {:.2f}'\n",
    "         .format(nbclf.score(X_train, y_train)))\n",
    "\n",
    "    print('Accuracy of GaussianNB classifier on test set: {:.2f}'\n",
    "         .format(nbclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB classifier with k-cross = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.75      0.93      0.83      1360\n",
      "        Over       0.80      0.48      0.60       802\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.78      0.70      0.72      2162\n",
      "weighted avg       0.77      0.76      0.75      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.75      0.94      0.83      1355\n",
      "        Over       0.82      0.47      0.59       807\n",
      "\n",
      "    accuracy                           0.76      2162\n",
      "   macro avg       0.78      0.70      0.71      2162\n",
      "weighted avg       0.77      0.76      0.74      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.74      0.93      0.82      1348\n",
      "        Over       0.79      0.47      0.59       814\n",
      "\n",
      "    accuracy                           0.75      2162\n",
      "   macro avg       0.77      0.70      0.71      2162\n",
      "weighted avg       0.76      0.75      0.74      2162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.73      0.92      0.82      1367\n",
      "        Over       0.76      0.43      0.55       794\n",
      "\n",
      "    accuracy                           0.74      2161\n",
      "   macro avg       0.75      0.67      0.68      2161\n",
      "weighted avg       0.74      0.74      0.72      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.76      0.92      0.83      1408\n",
      "        Over       0.76      0.44      0.56       753\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.76      0.68      0.70      2161\n",
      "weighted avg       0.76      0.76      0.74      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.76      0.93      0.84      1370\n",
      "        Over       0.81      0.49      0.61       791\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.78      0.71      0.73      2161\n",
      "weighted avg       0.78      0.77      0.76      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.77      0.91      0.83      1394\n",
      "        Over       0.76      0.49      0.60       767\n",
      "\n",
      "    accuracy                           0.76      2161\n",
      "   macro avg       0.76      0.70      0.71      2161\n",
      "weighted avg       0.76      0.76      0.75      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.74      0.92      0.82      1355\n",
      "        Over       0.77      0.46      0.58       806\n",
      "\n",
      "    accuracy                           0.75      2161\n",
      "   macro avg       0.76      0.69      0.70      2161\n",
      "weighted avg       0.75      0.75      0.73      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.73      0.93      0.82      1349\n",
      "        Over       0.78      0.43      0.55       812\n",
      "\n",
      "    accuracy                           0.74      2161\n",
      "   macro avg       0.75      0.68      0.68      2161\n",
      "weighted avg       0.75      0.74      0.72      2161\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Under       0.76      0.92      0.84      1388\n",
      "        Over       0.78      0.48      0.60       773\n",
      "\n",
      "    accuracy                           0.77      2161\n",
      "   macro avg       0.77      0.70      0.72      2161\n",
      "weighted avg       0.77      0.77      0.75      2161\n",
      "\n",
      "Avg precision: 0.7617639881013315\n",
      "Avg recall: 0.7565349666379999\n",
      "Accuracy: 0.7565349666379999\n"
     ]
    }
   ],
   "source": [
    "# Model performance using k-cross\n",
    "nbclf2 = GaussianNB()\n",
    "\n",
    "precision_sum = recall_sum = accuracy_sum = 0\n",
    "\n",
    "metrics_df = pd.DataFrame(columns = ['precision', 'recall', 'accuracy'])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # for each iteration, get training data and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # train the model using training data\n",
    "    nbclf2.fit(X_train, y_train)\n",
    "    \n",
    "    # predict y values using test data\n",
    "    y_pred = nbclf2.predict(X_test)\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    #print(confusion_mat)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Since we can retrieve a dictionary of metrics and access the values using dictionary,\n",
    "    # now we can sum of the results of each iteration and get the average\n",
    "    result_metrics_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    #print(result_metrics_dict)\n",
    "    \n",
    "    precision_sum += result_metrics_dict[\"weighted avg\"][\"precision\"]\n",
    "    recall_sum += result_metrics_dict[\"weighted avg\"][\"recall\"]\n",
    "    accuracy_sum += result_metrics_dict[\"accuracy\"]\n",
    "    \n",
    "    metrics_list = []\n",
    "    metrics_list.append(result_metrics_dict[\"weighted avg\"][\"precision\"])\n",
    "    metrics_list.append(result_metrics_dict[\"weighted avg\"][\"recall\"])\n",
    "    metrics_list.append(result_metrics_dict[\"accuracy\"])\n",
    "    \n",
    "    metrics_series = pd.Series(metrics_list, index=metrics_df.columns)\n",
    "    metrics_df = metrics_df.append(metrics_series, ignore_index=True)\n",
    "\n",
    "print(\"Avg precision:\", precision_sum/kf.get_n_splits(X))\n",
    "print(\"Avg recall:\", recall_sum/kf.get_n_splits(X))\n",
    "print(\"Accuracy:\", accuracy_sum/kf.get_n_splits(X))\n",
    "\n",
    "metrics_df.to_csv('./MLP3_data/nb_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.823842025882168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.20)\n",
    "\n",
    "y_score = nbclf2.predict_proba(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_score[:,1])\n",
    "\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('Accuracy = ', roc_auc)\n",
    "\n",
    "# Plotting\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=('Accuracy = %0.2f'%roc_auc))\n",
    "plt.legend(loc='lower right', prop={'size':8})\n",
    "plt.plot([0,1],[0,1], color='lightgrey', linestyle='--')\n",
    "plt.xlim([-0.05,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
